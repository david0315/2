<html><head><meta charset='utf-8'><meta name='viewport' content='width=device-width, initial-scale=1'>
<meta name='applicable-device' content='pc'><meta name='keywords' content='电脑,电脑讲解,电脑技术,编程,电脑故障维修我的第一个开源项目' />
<script src='../../highlight/highlight.pack.js'></script>
<link rel='stylesheet' type='text/css' href='../../highlight/styles/monokai.css'/>

<link rel='stylesheet' href='../../fenxiang/dist/css/share.min.css'>
<script src='../../fenxiang/src/js/social-share.js'></script>
<script src='../../fenxiang/src/js/qrcode.js'></script>

</head><body><script>hljs.initHighlightingOnLoad();</script><script>
var system ={};  
var p = navigator.platform;       
system.win = p.indexOf('Win') == 0;  
system.mac = p.indexOf('Mac') == 0;  
system.x11 = (p == 'X11') || (p.indexOf('Linux') == 0);     
if(system.win||system.mac||system.xll){
document.write("<link href='../css/3.css' rel='stylesheet' type='text/css'>");}else{ document.write("<link href='../css/3wap.css' rel='stylesheet' type='text/css'>");}</script><script src='../../js/3.js'></script><div class='div2'><div class='heading_nav'><ul><div><li><a href='../../index.html'>首页</a></li>
</div><div onclick='hidden1()' >分享</div>
</ul></div></div>
<div id='heading_nav2'> 
<li class='row' >
<div class='social-share' data-mode='prepend'><a href='javascript:' class='social-share-icon icon-heart'></a></div></li></div><script charset='utf-8' src='../../3/js/hengfu.js'></script><script charset='utf-8' src='../../3/js/hengfu2.js'></script><hr><div class='div1'><div class='biaoti'><center>我的第一个开源项目</center></div><div class='banquan'>原文出处:本文由博客园博主眺望小寒山提供。<br/>
原文连接:https://www.cnblogs.com/pdzbokey/p/11639779.html</div><br>
    <h2 id="地址">地址</h2>
<p><a href="https://gitee.com/ospdz/catclaw">gitee地址</a>，GitHub网速较慢，没有上传到GitHub</p>
<h2 id="项目介绍">项目介绍</h2>
<ol>
<li><p>一个简单的多线程爬虫，具有断点爬取功能，以及自动推测网页编码功能</p></li>
<li><p>使用jdk11+,maven构建，我使用jdk11.0.2时会报错，见<a href="https://bugs.openjdk.java.net/browse/JDK-8213202" class="uri">https://bugs.openjdk.java.net/browse/JDK-8213202</a>，jdk8却没有，jdk本身的bug，所以我换了最新版的jdk11.0.4</p></li>
<li><p>解析网页主要使用了Jsoup和正则表达式，所以这两个需要会用</p></li>
</ol>
<h2 id="思路">思路</h2>
<ol>
<li><p>先有一个或多个具体的初始链接，假如说把这些初始链接的页面类型记为seed</p></li>
<li><p>定义一个规则，规则主要由Jsoup选择器(类似于css选择器)、正则表达式、页面类型type组成，选择器一般定位到a标签。seed页面通过应用规则就可以自动获取标签中符合正则表达式的链接，并将这些链接存储到h2中，这些链接就是下一层要爬取的链接，假如将这些链接所代表的页面记为page1(type设置成了page1)</p></li>
<li><p>重复上一步，也是定义规则，唯一的区别就是应用这些规则的页面不是seed，而是page1页面。也就是说这些规则是有顺序的，除了第一个规则应用于seed页面之外，其余的每一个规则都是应用于上一个规则获取到的页面，以此类推，多少层都可以</p></li>
<li><p>结合示例查看源代码能更快的理解</p></li>
</ol>
<h2 id="示例及说明">示例及说明</h2>
<pre><code><code>ZongHeng zh = new ZongHeng();
zh.setCacheDirectory(&quot;f:/spider&quot;);
zh.config().setInterval(50).setBreakPoint(false);
//添加一个初始链接
zh.addSeed(&quot;http://www.zongheng.com/&quot;);
//添加一个规则，按照这个规则在初始链接页面取链接，并将取到的链接代表的页面记为category
zh.addRule(&quot;ul#data-groupdom li a&quot;, &quot;http://www.zongheng.com/category/\\d+.html&quot;, &quot;category&quot;);
//添加规则，在category页面取链接
zh.addRegex(&quot;http://book.zongheng.com/book/\\d+.html&quot;, &quot;book&quot;);
zh.start();</code></code></pre>
<p>每一个具体的爬虫都继承自Spider类，上面的代码就定义了一个简单的爬虫。</p>
<ol>
<li>第2行：设置缓存目录f:/spider</li>
<li>第3行：提交任务间隔最大50毫秒，关闭断点爬取</li>
<li>第4行：添加一个初始链接</li>
<li>第5行：添加一个规则，按照这个规则在初始链接页面取链接，并将取到的链接代表的页面记为category</li>
<li>第6行：添加一个规则，按照这个规则在category页面取链接，并将取到的链接代表的页面记为category</li>
<li>第7行：启动爬虫</li>
</ol>
<p>启动后爬虫就能自己去获取符合规则的网页，但获取网页不是我们的最终目的，我们的最终目的是从网页中提取自己需要的信息，所以继承Spider后还有一个必须重写的方法</p>
<pre><code><code>@Override
public void parse(Page page) {
    if (!page.isSeed()) {
        String text = page.urlText();
        System.out.println(text + &quot;---&quot; + page.link() + &quot;---&quot; + page.prevLink());
    }
    if (page.typeEquals(&quot;category&quot;)) {
        //category页面取得的下层链接的标签的文本去掉空白后长度大于1
        page.setNextLinksFilter(e -&gt; e.hasText() &amp;&amp; e.text().replaceAll(&quot;\\s+&quot;, &quot;&quot;).length() &gt; 1);
    }
}</code></code></pre>
<p>解析主要就是应用Jsoup里面的方法，通过page.getDoc()方法可以获取org.jsoup.nodes.Document对象，或者直接调用page.select(String cssQuery)方法解析，这就不多说了。</p>
<p>要说一下就是Page里面的3个高阶函数，所谓高阶函数就是接收函数作为参数的函数，这3个函数分别是setNextLinksFilter，setNextInfoFunc，setNextLinkUnary，作用分别是筛选符合条件的链接作为下层链接，携带信息到下一层，链接映射。</p>
<p>上面的代码就用到了筛选的方法，传入的函数是这样的：函数的参数是org.jsoup.nodes.Element类型的对象，一般代表a标签，函数返回boolean类型，返回值为true的Element才能作为下一层链接的提供者。</p>
<h2 id="手动获取下层链接">手动获取下层链接</h2>
<p>上面说的是自动获取下层链接，即只需要通过规则爬虫就能一层一层的获取链接，但有时候这样可能行不通，需要手动的获取链接然后添加到下一层，怎么做呢？</p>
<p>看一下这个<a href="https://gitee.com/ospdz/catclaw/blob/master/src/main/java/top/zheng/example/ZongHeng1.java">例子</a>，这其实完全可以自动获取下层链接，为了演示改成了手动获取下层链接，addByHand(String type)方法看源码知道这也是一个规则，只不过是一个只有type属性的规则而已。</p>
<pre><code><code>//添加一个初始链接
zh.addSeed(&quot;http://www.zongheng.com/&quot;);
zh.addByHand(&quot;category&quot;);</code></code></pre>
<p>可以看到初始链接后面紧跟一个手动添加规则，记为category</p>
<pre><code><code>if (page.isSeed()) {
    //这样还是用规则获取的下层链接，所以这没必要用手动，这里仅作为示例
    page.addNext(Rule.createRule(&quot;ul#data-groupdom li a&quot;, &quot;http://www.zongheng.com/category/\\d+.html&quot;));
}</code></code></pre>
<p>然后在seed页面取链接(这些链接所表示的页面就是category页面)加入到下一层，这就是手动添加了</p>
<h2 id="局限">局限</h2>
<ul>
<li><p>由于自动缓存的原因，编写好规则并爬取后，如果再次修改规则，如加入筛选条件等，由于从缓存中取数据，此时筛选条件对已经爬取的网页不起作用，要使它起作用，请删除缓存</p></li>
<li><p>目前只支持get请求或能转换成get请求的链接</p></li>
</ul>
<h2 id="感谢">感谢</h2>
<ul>
<li><a href="https://github.com/biezhi/elves">elves</a>：参考了该项目思想</li>
<li><a href="https://gitee.com/webcollector/WebCollector">WebCollector</a>：参考了该项目思想</li>
<li><a href="https://jsoup.org/">Jsoup</a>：用于下载和解析网页</li>
<li><a href="http://h2database.com/html/main.html">h2 database</a>：提供缓存</li>
<li><a href="https://commons.apache.org/proper/commons-dbutils/">DbUtils</a>：操作数据库</li>
</ul>


</div><hr><script charset='utf-8' src='../../js/sming.js'></script></body></html>