<html><head><meta charset='utf-8'><meta name='viewport' content='width=device-width, initial-scale=1'>
<meta name='applicable-device' content='pc'><meta name='keywords' content='电脑,电脑讲解,电脑技术,编程,电脑故障维修通俗地说逻辑回归【Logistic regression】算法（一） 逻辑回归模型原理介绍' />
<script src='../../highlight/highlight.pack.js'></script>
<link rel='stylesheet' type='text/css' href='../../highlight/styles/monokai.css'/>

<link rel='stylesheet' href='../../fenxiang/dist/css/share.min.css'>
<script src='../../fenxiang/src/js/social-share.js'></script>
<script src='../../fenxiang/src/js/qrcode.js'></script>

</head><body><script>hljs.initHighlightingOnLoad();</script><script>
var system ={};  
var p = navigator.platform;       
system.win = p.indexOf('Win') == 0;  
system.mac = p.indexOf('Mac') == 0;  
system.x11 = (p == 'X11') || (p.indexOf('Linux') == 0);     
if(system.win||system.mac||system.xll){
document.write("<link href='../css/3.css' rel='stylesheet' type='text/css'>");}else{ document.write("<link href='../css/3wap.css' rel='stylesheet' type='text/css'>");}</script><script src='../../js/3.js'></script><div class='div2'><div class='heading_nav'><ul><div><li><a href='../../index.html'>首页</a></li>
</div><div onclick='hidden1()' >分享</div>
</ul></div></div>
<div id='heading_nav2'> 
<li class='row' >
<div class='social-share' data-mode='prepend'><a href='javascript:' class='social-share-icon icon-heart'></a></div></li></div><script charset='utf-8' src='../../3/js/hengfu.js'></script><script charset='utf-8' src='../../3/js/hengfu2.js'></script><hr><div class='div1'><div class='biaoti'><center>通俗地说逻辑回归【Logistic regression】算法（一） 逻辑回归模型原理介绍</center></div><div class='banquan'>原文出处:本文由博客园博主zzzzMing提供。<br/>
原文连接:https://www.cnblogs.com/listenfwind/p/11259227.html</div><br>
    <p>在说逻辑回归前，还是得提一提他的兄弟，线性回归。在某些地方，逻辑回归算法和线性回归算法是类似的。但它和线性回归最大的不同在于，逻辑回归是作用是分类的。</p>
<p>还记得之前说的吗，线性回归其实就是求出一条拟合空间中所有点的线。逻辑回归的本质其实也和线性回归一样，但它加了一个步骤，逻辑回归使用sigmoid函数转换线性回归的输出以返回概率值，然后可以将概率值映射到两个或更多个离散类。</p>
<p>如果给出学生的成绩，比较线性回归和逻辑回归的不同如下：</p>
<ul>
<li>线性回归可以帮助我们以0-100的等级预测学生的测试分数。线性回归预测是连续的（某个范围内的数字）。</li>
<li>Logistic回归可以帮助预测学生是否通过。逻辑回归预测是离散的（仅允许特定值或类别）。我们还可以查看模型分类背后的概率值。</li>
</ul>
<h1 id="一.从回归到分类的核心---sigmoid-function">一.从回归到分类的核心 --Sigmoid Function</h1>
<p>之前介绍线性回归的时候，它的函数是这样样子的：</p>
<blockquote>
<p>h(x)=θ0 + θ1 * x1 + θ2 * x2 + θ3 * x3 ...</p>
</blockquote>
<p>但这样的函数是没办法进行分类的工作的，所以我们要借助一下其他函数，那就是Sigmoid Function。</p>
<p>我们先来看看这个Sigmoid Function长什么样，Sigmoid Function的数学公式是这样子的：</p>
<p><img src="./images/通俗地说逻辑回归【Logistic regression】算法（一） 逻辑回归模型原理介绍0.png" alt="Sigmoid函数" /></p>
<p>如果表示在平面坐标轴上呢，那它长这个样子。<br />
<img src="./images/通俗地说逻辑回归【Logistic regression】算法（一） 逻辑回归模型原理介绍1.png" alt="Sigmoid函数的坐标轴" /></p>
<p><strong>这个Sigmoid Function可以将线性的值，映射到[0-1]这个范围中</strong>。如果映射结果小于0.5，则认为是负的样本，如果是大于0.5，则认为是正的样本。</p>
<p>比方说要对垃圾邮箱进行分类，分垃圾邮箱和正常邮箱。当这个Sigmoid Function的计算出来后，小于0.5，则认为是垃圾邮箱，大于0.5则是非垃圾邮箱。</p>
<p>原先线性回归的计算公式是这样的：<br />
<img src="./images/通俗地说逻辑回归【Logistic regression】算法（一） 逻辑回归模型原理介绍2.png" alt="线性回归公式" /></p>
<p>那么将这个z函数代入到Sigmoid Function中，OK，现在我们就有了一个逻辑回归的函数了。</p>
<p><img src="./images/通俗地说逻辑回归【Logistic regression】算法（一） 逻辑回归模型原理介绍3.png" alt="逻辑回归的公式" /></p>
<h1 id="二.代价函数cost-function">二.代价函数Cost Function</h1>
<p>和线性回归一样，逻辑回归也有代价函数。并且都是通过最小化Cost Function来求得最终解的。</p>
<p>我们先来看单个点的情况，<br />
<img src="./images/通俗地说逻辑回归【Logistic regression】算法（一） 逻辑回归模型原理介绍4.png" alt="代价函数" /></p>
<p>这个代价函数呢，叫做交叉熵，其中y(i)指的是预测的结果，而hθ(xi)指的是xi这个点原本的值。</p>
<p>那么它具体是什么意思呢，为什么叫做交叉熵？我们举两个极端的例子看看就明白了：</p>
<h4 id="xi原始值hθ1预测结果yi1的情况">1.xi原始值hθ=1，预测结果，yi=1的情况</h4>
<p><img src="./images/通俗地说逻辑回归【Logistic regression】算法（一） 逻辑回归模型原理介绍5.png" alt="极端情况下的Sigmoid函数1" /></p>
<p>这个时候，<strong>代价函数的加号右边会被消掉，因为右边（1-y(i)）是0</strong>，左边部分呢，因为hθ(xi)=1，故而log(1)=0。</p>
<blockquote>
<p>y(i)log(hθ(xi)) = 1 * log(0) = 0</p>
</blockquote>
<p>也就是说，若xi原始值是1，当预测值y=1的时候，代价函数是0的。这个也比较好理解，代价函数为0就是说预测结果和原始结果完全一致的，没有半点出差错。</p>
<h4 id="计算结果yi0原始值hθ0">2.计算结果，yi=0，原始值hθ=0</h4>
<p><img src="./images/通俗地说逻辑回归【Logistic regression】算法（一） 逻辑回归模型原理介绍6.png" alt="极端情况下的Sigmoid函数2" /></p>
<p>这次的结果就和上面的反过来了，因为yi=0，所以左边部分全军覆没，来看右边，</p>
<blockquote>
<p>(1-yi) * log(1-hθ(xi)) = 1 * log(0) = 0</p>
</blockquote>
<p>因为1-hθ(xi)，最终结果还是等于0。</p>
<p><strong>也就是说，这个损失函数，只要原始值与预测结果越相符，损失函数就越大，反之，损失函数就会越小。</strong></p>
<p>以上说的只是一个点的情况，实际的代价函数，是要计算所有点的损失函数的均值，如下所示：</p>
<p><img src="./images/通俗地说逻辑回归【Logistic regression】算法（一） 逻辑回归模型原理介绍7.png" alt="变换的损失函数" /></p>
<h1 id="三.梯度下降">三.梯度下降</h1>
<p>和线性回归一样，逻辑回归的解法也可以通过梯度下降来进行求解。梯度下降的目的，是为了最小化代价函数Cost function。</p>
<p>要求使用梯度下降，需要先求解偏导数，以下是求导数的一个具体过程：<br />
<img src="./images/通俗地说逻辑回归【Logistic regression】算法（一） 逻辑回归模型原理介绍8.png" alt="求导过程" /></p>
<p>而梯度下降的计算方法也和线性回归的计算方法是一样的。只是其中的代价函数，换成了逻辑回归的代价函数。</p>
<p><img src="./images/通俗地说逻辑回归【Logistic regression】算法（一） 逻辑回归模型原理介绍9.png" /></p>
<p>其中，α右边部分对应我们上面对代价函数求偏导的结果。而α是用来控制训练速率的，这个在线性回归那里已经有说到，这里就不再介绍了。</p>
<p>最终就是对θj不断迭代，直到损失函数降到最小，那就可以求出我们要的θ值了。</p>
<h1 id="四.小结">四.小结</h1>
<p>OK，今天介绍了线性回归和逻辑回归的区别，同样都是回归分析，逻辑回归能完成分类任何的核心，就算使用了Sigmoid Function。</p>
<p>这里留一个小问题，上面所述的逻辑回归，通常是仅仅能够进行二分类，那有没有办法来让逻辑回归实现多分类呢？</p>
<p>下一次将阐述用逻辑回归进行多分类，以及正则化相关内容，并介绍sklearn的逻辑回归参数和用法！！</p>
<p>以上~~</p>
<hr />
<p>推荐阅读：<br />
<a href="https://www.cnblogs.com/listenfwind/p/11180475.html">通俗得说线性回归算法（一）线性回归初步介绍</a><br />
<a href="https://www.cnblogs.com/listenfwind/p/11676301.html">通俗得说线性回归算法（二）线性回归初步介绍</a><br />
<a href="https://www.cnblogs.com/listenfwind/p/11209383.html">Scala 函数式编程指南（一） 函数式思想介绍</a><br />
<a href="https://www.cnblogs.com/listenfwind/p/10199720.html">通俗地说决策树算法（二）实例解析</a><br />
<a href="https://www.cnblogs.com/listenfwind/p/10133772.html">大数据存储的进化史 --从 RAID 到 Hadoop Hdfs</a><br />
<a href="https://www.cnblogs.com/listenfwind/p/10561277.html">C，java，Python，这些名字背后的江湖！</a></p>


</div><hr><script charset='utf-8' src='../../js/sming.js'></script></body></html>